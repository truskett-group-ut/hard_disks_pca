{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from analysis_tools.read_traj import ReadTraj\n",
    "from analysis_tools.feature_creation import FrameToFeaturesPosition, TrajectoryToFeaturesPosition\n",
    "from analysis_tools.feature_creation import FrameToFeaturesComposition, TrajectoryToFeaturesComposition\n",
    "from analysis_tools.radial_distribution_function import RDF, PositionalSuceptibility\n",
    "from analysis_tools.reservoir_sampler import ReservoirSampler\n",
    "from analysis_tools.defect_analysis import DefectStats\n",
    "from analysis_tools.pop2d import POP2D\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DownSampleFrames(frames, frame_samples):\n",
    "    return frames[0::max(len(frames)/frame_samples, 1)][0:frame_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import arange, array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#VERSION FOR PAPER I\n",
    "N_nn = 3200 #number of nearest neighbors for pca analysis\n",
    "split = 1 #chunks the data up so it can be processed by the pca tool if really large\n",
    "nn_inc = 10 #1 (full in paper) #reduces the number of nearest neighbors to include as features\n",
    "remove_types = []\n",
    "shuffle_data = True\n",
    "N_batch = 1\n",
    "batches_per_frame = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify what data to read in and process\n",
    "traj_type = 'gsd'\n",
    "file_data = [(arange(0.550, 0.690001, 0.005), '../hoomd_disks/trajectories_4000p', 400), \n",
    "             (arange(0.695, 0.820001, 0.005),'../hoomd_disks/trajectories_4000p_longer', 400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#actual data\n",
    "etas = []\n",
    "[etas.extend(etas_) for etas_, _, _ in file_data]\n",
    "etas = array(etas)\n",
    "color_ids = []\n",
    "[color_ids.extend(len(data[0])*[count]) for count, data in zip(range(len(file_data)), file_data)]\n",
    "color_ids = array(color_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array_split\n",
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.55\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.555\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.56\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.565\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.57\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.575\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.58\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.585\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.59\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.595\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.6\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.605\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.61\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.615\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.62\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.625\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.63\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.635\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.64\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.645\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.65\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.655\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.66\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.665\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.67\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.675\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.68\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.685\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.69\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.695\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.7\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.705\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.71\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.715\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.72\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.725\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.73\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.735\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.74\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.745\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.75\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.755\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.76\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.765\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.77\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.775\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.78\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.785\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.79\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.795\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.8\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.805\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.81\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.815\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.82\n",
      "using 400 frames of 1000 total\n",
      "Fitting the whitener\n",
      "\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.55\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.555\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.56\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.565\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.57\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.575\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.58\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.585\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.59\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.595\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.6\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.605\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.61\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.615\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.62\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.625\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.63\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.635\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.645\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.65\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.655\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.66\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.665\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.67\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.675\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.68\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.685\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p\n",
      "eta = 0.69\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.695\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.7\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.705\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.71\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.715\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.72\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.725\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.73\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.735\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.74\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.745\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.75\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.755\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.76\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.765\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.77\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.775\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.78\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.785\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.79\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.795\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.8\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.805\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.81\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.815\n",
      "using 400 frames of 1000 total\n",
      "COMPUTATION DETAILS\n",
      "file_base = ../hoomd_disks/trajectories_4000p_longer\n",
      "eta = 0.82\n",
      "using 400 frames of 1000 total\n",
      "Correcting features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corrected_features = []\n",
    "incpca_ig = IncrementalPCA(n_components=None, whiten=True) \n",
    "force_randomize = False\n",
    "\n",
    "#loop over data sets \n",
    "for phase in ['fit_whitener', 'correct_features']:\n",
    "    raw_features = []\n",
    "    \n",
    "    for etas_, file_base, frame_samples in file_data:\n",
    "\n",
    "        for eta in etas_:\n",
    "            print 'COMPUTATION DETAILS'\n",
    "            print 'file_base = {}'.format(file_base)\n",
    "            print 'eta = {}'.format(eta)\n",
    "\n",
    "            #read in data and randomize positions if performing ideal gas correction\n",
    "            filename = \"{}/trajectory_{:.4f}.{}\".format(file_base, eta, traj_type)\n",
    "            randomize = (phase == 'fit_whitener') or force_randomize\n",
    "            frames = ReadTraj(filename, traj_type, shuffle_data, randomize, remove_types)\n",
    "\n",
    "            #control the number of total frames to analyze\n",
    "            len_frames_init = len(frames)\n",
    "            frames = DownSampleFrames(frames, frame_samples)\n",
    "            print 'using {} frames of {} total'.format(len(frames), len_frames_init)\n",
    "            \n",
    "            raw_features.extend(TrajectoryToFeaturesPosition(frames, \n",
    "                                                             N_nn=N_nn,  \n",
    "                                                             nn_inc=nn_inc,\n",
    "                                                             N_batch=N_batch, \n",
    "                                                             batches_per_frame=batches_per_frame))\n",
    "    if phase == 'fit_whitener':\n",
    "        print 'Fitting the whitener\\n'\n",
    "        incpca_ig.fit(raw_features)\n",
    "    else:\n",
    "        print 'Correcting features\\n'\n",
    "        corrected_features = incpca_ig.transform(raw_features)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from analysis_tools.autoencoder import AutoEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: 320\n"
     ]
    }
   ],
   "source": [
    "dim = corrected_features.shape[1]\n",
    "print \"Dimensionality: {}\".format(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrected_features_train, corrected_features_test = train_test_split(corrected_features, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AutoEncoder(dim=dim, encode_dim=1, \n",
    "                    initial_growth=0.25, shrink=0.65,\n",
    "                    activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 260, 169, 109, 70, 45, 29, 18, 11, 7, 4, 2, 1, 2, 4, 7, 11, 18, 29, 45, 70, 109, 169, 260, 400, 320]\n"
     ]
    }
   ],
   "source": [
    "print model.layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 260, 169, 109, 70, 45, 29, 18, 11, 7, 4, 2, 1, 2, 4, 7, 11, 18, 29, 45, 70, 109, 169, 260, 400, 320]\n"
     ]
    }
   ],
   "source": [
    "print model.layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.Compile(optimizer='adamax', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='./model/weights_elu_adam.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16500 samples, validate on 5500 samples\n",
      "Epoch 1/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8028Epoch 00000: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8027 - val_loss: 0.8089\n",
      "Epoch 2/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8028Epoch 00001: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8029 - val_loss: 0.8087\n",
      "Epoch 3/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00002: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8089\n",
      "Epoch 4/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00003: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8087\n",
      "Epoch 5/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00004: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8030 - val_loss: 0.8087\n",
      "Epoch 6/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00005: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8087\n",
      "Epoch 7/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00006: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8091\n",
      "Epoch 8/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00007: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8088\n",
      "Epoch 9/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00008: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8084\n",
      "Epoch 10/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8029Epoch 00009: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8029 - val_loss: 0.8087\n",
      "Epoch 11/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8031Epoch 00010: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8085\n",
      "Epoch 12/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00011: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8087\n",
      "Epoch 13/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00012: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8089\n",
      "Epoch 14/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00013: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8085\n",
      "Epoch 15/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00014: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8093\n",
      "Epoch 16/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00015: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8087\n",
      "Epoch 17/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8052Epoch 00016: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8094\n",
      "Epoch 18/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00017: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8094\n",
      "Epoch 19/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00018: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8081\n",
      "Epoch 20/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00019: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8084\n",
      "Epoch 21/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00020: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8082\n",
      "Epoch 22/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00021: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8085\n",
      "Epoch 23/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00022: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8089\n",
      "Epoch 24/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8053Epoch 00023: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8054 - val_loss: 0.8093\n",
      "Epoch 25/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00024: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8084\n",
      "Epoch 26/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00025: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8083\n",
      "Epoch 27/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8053Epoch 00026: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8054 - val_loss: 0.8090\n",
      "Epoch 28/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8054Epoch 00027: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8054 - val_loss: 0.8088\n",
      "Epoch 29/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8055Epoch 00028: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8054 - val_loss: 0.8101\n",
      "Epoch 30/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8057Epoch 00029: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8056 - val_loss: 0.8084\n",
      "Epoch 31/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00030: val_loss improved from 0.80808 to 0.80800, saving model to ./model/weights_elu_adam.hdf5\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8080\n",
      "Epoch 32/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00031: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8084\n",
      "Epoch 33/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00032: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8085\n",
      "Epoch 34/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00033: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8090\n",
      "Epoch 35/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8051Epoch 00034: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8086\n",
      "Epoch 36/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8050Epoch 00035: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8096\n",
      "Epoch 37/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8058Epoch 00036: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8057 - val_loss: 0.8092\n",
      "Epoch 38/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8052Epoch 00037: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8051 - val_loss: 0.8094\n",
      "Epoch 39/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00038: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8051 - val_loss: 0.8085\n",
      "Epoch 40/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8052Epoch 00039: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8084\n",
      "Epoch 41/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8051Epoch 00040: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8053 - val_loss: 0.8092\n",
      "Epoch 42/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00041: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8085\n",
      "Epoch 43/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00042: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8087\n",
      "Epoch 44/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00043: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8086\n",
      "Epoch 45/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8055Epoch 00044: val_loss did not improve\n",
      "16500/16500 [==============================] - 5s - loss: 0.8055 - val_loss: 0.8086\n",
      "Epoch 46/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00045: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8087\n",
      "Epoch 47/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8055Epoch 00046: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8052 - val_loss: 0.8086\n",
      "Epoch 48/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00047: val_loss improved from 0.80800 to 0.80759, saving model to ./model/weights_elu_adam.hdf5\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8076\n",
      "Epoch 49/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00048: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8085\n",
      "Epoch 50/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00049: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8079\n",
      "Epoch 51/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00050: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8079\n",
      "Epoch 52/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00051: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8089\n",
      "Epoch 53/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00052: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8091\n",
      "Epoch 54/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8057Epoch 00053: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8056 - val_loss: 0.8110\n",
      "Epoch 55/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8054Epoch 00054: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8054 - val_loss: 0.8083\n",
      "Epoch 56/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00055: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8081\n",
      "Epoch 57/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00056: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8051 - val_loss: 0.8086\n",
      "Epoch 58/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00057: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8084\n",
      "Epoch 59/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00058: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8083\n",
      "Epoch 60/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00059: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8080\n",
      "Epoch 61/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00060: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8089\n",
      "Epoch 62/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00061: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8085\n",
      "Epoch 63/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00062: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8084\n",
      "Epoch 64/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00063: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8080\n",
      "Epoch 65/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00064: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8088\n",
      "Epoch 66/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00065: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8083\n",
      "Epoch 67/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00066: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8080\n",
      "Epoch 68/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00067: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8090\n",
      "Epoch 69/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00068: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8079\n",
      "Epoch 70/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00069: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8086\n",
      "Epoch 71/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00070: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8086\n",
      "Epoch 72/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8057Epoch 00071: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8057 - val_loss: 0.8093\n",
      "Epoch 73/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00072: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8082\n",
      "Epoch 74/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00073: val_loss did not improve\n",
      "16500/16500 [==============================] - 5s - loss: 0.8044 - val_loss: 0.8083\n",
      "Epoch 75/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00074: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8083\n",
      "Epoch 76/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00075: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8083\n",
      "Epoch 77/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00076: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8100\n",
      "Epoch 78/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8055Epoch 00077: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8055 - val_loss: 0.8096\n",
      "Epoch 79/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8057Epoch 00078: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8056 - val_loss: 0.8093\n",
      "Epoch 80/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8055Epoch 00079: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 4s - loss: 0.8055 - val_loss: 0.8092\n",
      "Epoch 81/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00080: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8093\n",
      "Epoch 82/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8050Epoch 00081: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8094\n",
      "Epoch 83/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8050Epoch 00082: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8085\n",
      "Epoch 84/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00083: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8087\n",
      "Epoch 85/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00084: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8091\n",
      "Epoch 86/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00085: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8091\n",
      "Epoch 87/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8053Epoch 00086: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8053 - val_loss: 0.8097\n",
      "Epoch 88/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00087: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8081\n",
      "Epoch 89/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00088: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8082\n",
      "Epoch 90/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00089: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8079\n",
      "Epoch 91/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00090: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8078\n",
      "Epoch 92/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00091: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8081\n",
      "Epoch 93/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00092: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8083\n",
      "Epoch 94/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00093: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8087\n",
      "Epoch 95/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00094: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8083\n",
      "Epoch 96/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00095: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8085\n",
      "Epoch 97/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00096: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8078\n",
      "Epoch 98/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00097: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8092\n",
      "Epoch 99/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00098: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8081\n",
      "Epoch 100/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00099: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8081\n",
      "Epoch 101/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00100: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8081\n",
      "Epoch 102/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00101: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8085\n",
      "Epoch 103/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00102: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8088\n",
      "Epoch 104/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00103: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8083\n",
      "Epoch 105/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00104: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8086\n",
      "Epoch 106/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00105: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8088\n",
      "Epoch 107/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00106: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8087\n",
      "Epoch 108/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00107: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8081\n",
      "Epoch 109/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00108: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8084\n",
      "Epoch 110/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00109: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8082\n",
      "Epoch 111/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00110: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8083\n",
      "Epoch 112/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00111: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8100\n",
      "Epoch 113/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00112: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8078\n",
      "Epoch 114/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00113: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8091\n",
      "Epoch 115/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8054Epoch 00114: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8051 - val_loss: 0.8095\n",
      "Epoch 116/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8053Epoch 00115: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8054 - val_loss: 0.8096\n",
      "Epoch 117/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8051Epoch 00116: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8083\n",
      "Epoch 118/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00117: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8084\n",
      "Epoch 119/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00118: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8081\n",
      "Epoch 120/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00119: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8081\n",
      "Epoch 121/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00120: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8079\n",
      "Epoch 122/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00121: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8082\n",
      "Epoch 123/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00122: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8088\n",
      "Epoch 124/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00123: val_loss improved from 0.80759 to 0.80758, saving model to ./model/weights_elu_adam.hdf5\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8076\n",
      "Epoch 125/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00124: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8085\n",
      "Epoch 126/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00125: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8085\n",
      "Epoch 127/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00126: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8085\n",
      "Epoch 128/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00127: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8084\n",
      "Epoch 129/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00128: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8084\n",
      "Epoch 130/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00129: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8083\n",
      "Epoch 131/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00130: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8083\n",
      "Epoch 132/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00131: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8082\n",
      "Epoch 133/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00132: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8081\n",
      "Epoch 134/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00133: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8089\n",
      "Epoch 135/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00134: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8087\n",
      "Epoch 136/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00135: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8081\n",
      "Epoch 137/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00136: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8104\n",
      "Epoch 138/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00137: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8090\n",
      "Epoch 139/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00138: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8084\n",
      "Epoch 140/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00139: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8082\n",
      "Epoch 141/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00140: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8082\n",
      "Epoch 142/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00141: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8080\n",
      "Epoch 143/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00142: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8080\n",
      "Epoch 144/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00143: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8083\n",
      "Epoch 145/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00144: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8078\n",
      "Epoch 146/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00145: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8082\n",
      "Epoch 147/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00146: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8089\n",
      "Epoch 148/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00147: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8079\n",
      "Epoch 149/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00148: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8083\n",
      "Epoch 150/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00149: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8085\n",
      "Epoch 151/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00150: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8088\n",
      "Epoch 152/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00151: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8086\n",
      "Epoch 153/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00152: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8087\n",
      "Epoch 154/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8056Epoch 00153: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8056 - val_loss: 0.8091\n",
      "Epoch 155/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8053Epoch 00154: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8053 - val_loss: 0.8091\n",
      "Epoch 156/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8051Epoch 00155: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8051 - val_loss: 0.8085\n",
      "Epoch 157/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00156: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8086\n",
      "Epoch 158/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8053Epoch 00157: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8053 - val_loss: 0.8095\n",
      "Epoch 159/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8057Epoch 00158: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8057 - val_loss: 0.8105\n",
      "Epoch 160/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8072Epoch 00159: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8072 - val_loss: 0.8097\n",
      "Epoch 161/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8061Epoch 00160: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8063 - val_loss: 0.8095\n",
      "Epoch 162/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8060Epoch 00161: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8059 - val_loss: 0.8091\n",
      "Epoch 163/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8054Epoch 00162: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8055 - val_loss: 0.8093\n",
      "Epoch 164/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8063Epoch 00163: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8061 - val_loss: 0.8097\n",
      "Epoch 165/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8057Epoch 00164: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8058 - val_loss: 0.8110\n",
      "Epoch 166/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8060Epoch 00165: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8059 - val_loss: 0.8094\n",
      "Epoch 167/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8052Epoch 00166: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8088\n",
      "Epoch 168/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00167: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8088\n",
      "Epoch 169/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00168: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8085\n",
      "Epoch 170/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00169: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8087\n",
      "Epoch 171/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00170: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8090\n",
      "Epoch 172/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00171: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8084\n",
      "Epoch 173/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00172: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8080\n",
      "Epoch 174/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00173: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8087\n",
      "Epoch 175/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00174: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8082\n",
      "Epoch 176/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00175: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8079\n",
      "Epoch 177/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00176: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8079\n",
      "Epoch 178/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00177: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8083\n",
      "Epoch 179/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00178: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8078\n",
      "Epoch 180/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00179: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8084\n",
      "Epoch 181/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00180: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8085\n",
      "Epoch 182/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00181: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8078\n",
      "Epoch 183/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00182: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8081\n",
      "Epoch 184/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00183: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8079\n",
      "Epoch 185/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00184: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8079\n",
      "Epoch 186/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00185: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8082\n",
      "Epoch 187/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00186: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8089\n",
      "Epoch 188/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8056Epoch 00187: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8057 - val_loss: 0.8095\n",
      "Epoch 189/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8058Epoch 00188: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8057 - val_loss: 0.8089\n",
      "Epoch 190/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00189: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8096\n",
      "Epoch 191/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8051Epoch 00190: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8051 - val_loss: 0.8081\n",
      "Epoch 192/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00191: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8093\n",
      "Epoch 193/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00192: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8087\n",
      "Epoch 194/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00193: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8088\n",
      "Epoch 195/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00194: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8090\n",
      "Epoch 196/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00195: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8088\n",
      "Epoch 197/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00196: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8088\n",
      "Epoch 198/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00197: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8078\n",
      "Epoch 199/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00198: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8082\n",
      "Epoch 200/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00199: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8082\n",
      "Epoch 201/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00200: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8080\n",
      "Epoch 202/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00201: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8085\n",
      "Epoch 203/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00202: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8091\n",
      "Epoch 204/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8054Epoch 00203: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8053 - val_loss: 0.8093\n",
      "Epoch 205/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8050Epoch 00204: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8052 - val_loss: 0.8090\n",
      "Epoch 206/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8063Epoch 00205: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8062 - val_loss: 0.8105\n",
      "Epoch 207/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8064Epoch 00206: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8062 - val_loss: 0.8100\n",
      "Epoch 208/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8062Epoch 00207: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8063 - val_loss: 0.8100\n",
      "Epoch 209/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8060Epoch 00208: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8061 - val_loss: 0.8099\n",
      "Epoch 210/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8052Epoch 00209: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8052 - val_loss: 0.8088\n",
      "Epoch 211/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8050Epoch 00210: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8093\n",
      "Epoch 212/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00211: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8082\n",
      "Epoch 213/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00212: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8080\n",
      "Epoch 214/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00213: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8082\n",
      "Epoch 215/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00214: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8078\n",
      "Epoch 216/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00215: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8083\n",
      "Epoch 217/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00216: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8090\n",
      "Epoch 218/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00217: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8083\n",
      "Epoch 219/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00218: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8088\n",
      "Epoch 220/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00219: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8085\n",
      "Epoch 221/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00220: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8100\n",
      "Epoch 222/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00221: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8082\n",
      "Epoch 223/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00222: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8083\n",
      "Epoch 224/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00223: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8081\n",
      "Epoch 225/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00224: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8091\n",
      "Epoch 226/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00225: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8086\n",
      "Epoch 227/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00226: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8084\n",
      "Epoch 228/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00227: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8082\n",
      "Epoch 229/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00228: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8078\n",
      "Epoch 230/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00229: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8079\n",
      "Epoch 231/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00230: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8082\n",
      "Epoch 232/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00231: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8080\n",
      "Epoch 233/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00232: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8084\n",
      "Epoch 234/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00233: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8078\n",
      "Epoch 235/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00234: val_loss improved from 0.80758 to 0.80728, saving model to ./model/weights_elu_adam.hdf5\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8073\n",
      "Epoch 236/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00235: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8080\n",
      "Epoch 237/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00236: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8076\n",
      "Epoch 238/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00237: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8083\n",
      "Epoch 239/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8053Epoch 00238: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 4s - loss: 0.8053 - val_loss: 0.8100\n",
      "Epoch 240/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00239: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8083\n",
      "Epoch 241/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00240: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8081\n",
      "Epoch 242/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00241: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8087\n",
      "Epoch 243/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00242: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8086\n",
      "Epoch 244/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00243: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8083\n",
      "Epoch 245/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00244: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8092\n",
      "Epoch 246/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00245: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8092\n",
      "Epoch 247/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00246: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8084\n",
      "Epoch 248/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00247: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8087\n",
      "Epoch 249/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00248: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8089\n",
      "Epoch 250/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00249: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8097\n",
      "Epoch 251/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8059Epoch 00250: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8060 - val_loss: 0.8098\n",
      "Epoch 252/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8058Epoch 00251: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8059 - val_loss: 0.8099\n",
      "Epoch 253/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8063Epoch 00252: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8064 - val_loss: 0.8106\n",
      "Epoch 254/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8073Epoch 00253: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8072 - val_loss: 0.8108\n",
      "Epoch 255/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8056Epoch 00254: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8057 - val_loss: 0.8093\n",
      "Epoch 256/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00255: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8090\n",
      "Epoch 257/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8047Epoch 00256: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8092\n",
      "Epoch 258/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00257: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8092\n",
      "Epoch 259/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00258: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8088\n",
      "Epoch 260/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00259: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8083\n",
      "Epoch 261/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00260: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8091\n",
      "Epoch 262/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00261: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8091\n",
      "Epoch 263/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00262: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8092\n",
      "Epoch 264/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00263: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8088\n",
      "Epoch 265/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00264: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8089\n",
      "Epoch 266/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00265: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8082\n",
      "Epoch 267/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00266: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8092\n",
      "Epoch 268/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00267: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8047 - val_loss: 0.8084\n",
      "Epoch 269/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00268: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8091\n",
      "Epoch 270/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00269: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8090\n",
      "Epoch 271/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00270: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8086\n",
      "Epoch 272/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00271: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8093\n",
      "Epoch 273/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00272: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8086\n",
      "Epoch 274/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00273: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8042 - val_loss: 0.8086\n",
      "Epoch 275/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00274: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8091\n",
      "Epoch 276/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00275: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8081\n",
      "Epoch 277/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00276: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8084\n",
      "Epoch 278/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00277: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8087\n",
      "Epoch 279/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00278: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8077\n",
      "Epoch 280/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00279: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8087\n",
      "Epoch 281/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00280: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8084\n",
      "Epoch 282/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00281: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8083\n",
      "Epoch 283/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00282: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8086\n",
      "Epoch 284/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8038Epoch 00283: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8039 - val_loss: 0.8086\n",
      "Epoch 285/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00284: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8075\n",
      "Epoch 286/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00285: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8075\n",
      "Epoch 287/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8026Epoch 00286: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8028 - val_loss: 0.8073\n",
      "Epoch 288/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00287: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8075\n",
      "Epoch 289/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00288: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8030 - val_loss: 0.8079\n",
      "Epoch 290/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00289: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8081\n",
      "Epoch 291/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00290: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8077\n",
      "Epoch 292/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8026Epoch 00291: val_loss improved from 0.80728 to 0.80720, saving model to ./model/weights_elu_adam.hdf5\n",
      "16500/16500 [==============================] - 4s - loss: 0.8028 - val_loss: 0.8072\n",
      "Epoch 293/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8024Epoch 00292: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8026 - val_loss: 0.8077\n",
      "Epoch 294/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8031Epoch 00293: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8082\n",
      "Epoch 295/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00294: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8080\n",
      "Epoch 296/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00295: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8084\n",
      "Epoch 297/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8054Epoch 00296: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8054 - val_loss: 0.8093\n",
      "Epoch 298/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00297: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8086\n",
      "Epoch 299/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00298: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8097\n",
      "Epoch 300/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00299: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8046 - val_loss: 0.8093\n",
      "Epoch 301/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00300: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8085\n",
      "Epoch 302/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00301: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8084\n",
      "Epoch 303/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00302: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8082\n",
      "Epoch 304/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00303: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8081\n",
      "Epoch 305/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00304: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8078\n",
      "Epoch 306/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8031Epoch 00305: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8079\n",
      "Epoch 307/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8028Epoch 00306: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8029 - val_loss: 0.8081\n",
      "Epoch 308/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00307: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8084\n",
      "Epoch 309/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00308: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8078\n",
      "Epoch 310/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00309: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8087\n",
      "Epoch 311/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00310: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8085\n",
      "Epoch 312/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00311: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8086\n",
      "Epoch 313/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00312: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8090\n",
      "Epoch 314/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00313: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8038 - val_loss: 0.8080\n",
      "Epoch 315/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8031Epoch 00314: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8091\n",
      "Epoch 316/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8045Epoch 00315: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8096\n",
      "Epoch 317/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00316: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8090\n",
      "Epoch 318/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00317: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8082\n",
      "Epoch 319/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00318: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8080\n",
      "Epoch 320/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00319: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8079\n",
      "Epoch 321/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00320: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8076\n",
      "Epoch 322/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00321: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8027 - val_loss: 0.8079\n",
      "Epoch 323/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8031Epoch 00322: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8079\n",
      "Epoch 324/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00323: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8075\n",
      "Epoch 325/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00324: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8073\n",
      "Epoch 326/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00325: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8030 - val_loss: 0.8082\n",
      "Epoch 327/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8031Epoch 00326: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8073\n",
      "Epoch 328/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00327: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8077\n",
      "Epoch 329/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00328: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8077\n",
      "Epoch 330/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00329: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8081\n",
      "Epoch 331/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00330: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8086\n",
      "Epoch 332/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00331: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8082\n",
      "Epoch 333/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00332: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8078\n",
      "Epoch 334/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00333: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8079\n",
      "Epoch 335/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00334: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8082\n",
      "Epoch 336/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00335: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8078\n",
      "Epoch 337/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8027Epoch 00336: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8029 - val_loss: 0.8076\n",
      "Epoch 338/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8029Epoch 00337: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8029 - val_loss: 0.8080\n",
      "Epoch 339/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8031Epoch 00338: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8081\n",
      "Epoch 340/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00339: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8082\n",
      "Epoch 341/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00340: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8085\n",
      "Epoch 342/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00341: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8034 - val_loss: 0.8085\n",
      "Epoch 343/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00342: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8076\n",
      "Epoch 344/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8028Epoch 00343: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8026 - val_loss: 0.8075\n",
      "Epoch 345/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8025Epoch 00344: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8028 - val_loss: 0.8076\n",
      "Epoch 346/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8027Epoch 00345: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8027 - val_loss: 0.8081\n",
      "Epoch 347/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8029Epoch 00346: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8028 - val_loss: 0.8077\n",
      "Epoch 348/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8029Epoch 00347: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8028 - val_loss: 0.8079\n",
      "Epoch 349/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00348: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8088\n",
      "Epoch 350/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00349: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8030 - val_loss: 0.8086\n",
      "Epoch 351/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8026Epoch 00350: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8028 - val_loss: 0.8080\n",
      "Epoch 352/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8024Epoch 00351: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8025 - val_loss: 0.8076\n",
      "Epoch 353/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8024Epoch 00352: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8025 - val_loss: 0.8075\n",
      "Epoch 354/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8023Epoch 00353: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8022 - val_loss: 0.8082\n",
      "Epoch 355/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8026Epoch 00354: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8027 - val_loss: 0.8081\n",
      "Epoch 356/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8025Epoch 00355: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8026 - val_loss: 0.8073\n",
      "Epoch 357/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8026Epoch 00356: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8027 - val_loss: 0.8077\n",
      "Epoch 358/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00357: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8081\n",
      "Epoch 359/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8037Epoch 00358: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8037 - val_loss: 0.8083\n",
      "Epoch 360/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00359: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8079\n",
      "Epoch 361/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00360: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8082\n",
      "Epoch 362/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00361: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8086\n",
      "Epoch 363/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00362: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8084\n",
      "Epoch 364/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00363: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8088\n",
      "Epoch 365/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00364: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8091\n",
      "Epoch 366/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8046Epoch 00365: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8101\n",
      "Epoch 367/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00366: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8084\n",
      "Epoch 368/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8039Epoch 00367: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8077\n",
      "Epoch 369/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8031Epoch 00368: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8076\n",
      "Epoch 370/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8029Epoch 00369: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8029 - val_loss: 0.8085\n",
      "Epoch 371/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00370: val_loss did not improve\n",
      "16500/16500 [==============================] - 5s - loss: 0.8030 - val_loss: 0.8083\n",
      "Epoch 372/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8036Epoch 00371: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8097\n",
      "Epoch 373/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8050Epoch 00372: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8051 - val_loss: 0.8095\n",
      "Epoch 374/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8044Epoch 00373: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8045 - val_loss: 0.8088\n",
      "Epoch 375/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8042Epoch 00374: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8090\n",
      "Epoch 376/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8053Epoch 00375: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8052 - val_loss: 0.8096\n",
      "Epoch 377/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00376: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8043 - val_loss: 0.8088\n",
      "Epoch 378/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00377: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8087\n",
      "Epoch 379/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8035Epoch 00378: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8036 - val_loss: 0.8081\n",
      "Epoch 380/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8033Epoch 00379: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8082\n",
      "Epoch 381/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00380: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8033 - val_loss: 0.8088\n",
      "Epoch 382/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8032Epoch 00381: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8085\n",
      "Epoch 383/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8029Epoch 00382: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8031 - val_loss: 0.8083\n",
      "Epoch 384/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00383: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8092\n",
      "Epoch 385/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8030Epoch 00384: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8032 - val_loss: 0.8080\n",
      "Epoch 386/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8034Epoch 00385: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8035 - val_loss: 0.8087\n",
      "Epoch 387/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8041Epoch 00386: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8041 - val_loss: 0.8088\n",
      "Epoch 388/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8040Epoch 00387: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8040 - val_loss: 0.8092\n",
      "Epoch 389/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00388: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8048 - val_loss: 0.8094\n",
      "Epoch 390/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00389: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8094\n",
      "Epoch 391/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8048Epoch 00390: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8101\n",
      "Epoch 392/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8051Epoch 00391: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8052 - val_loss: 0.8098\n",
      "Epoch 393/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00392: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8098\n",
      "Epoch 394/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8043Epoch 00393: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8044 - val_loss: 0.8095\n",
      "Epoch 395/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00394: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8049 - val_loss: 0.8106\n",
      "Epoch 396/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8055Epoch 00395: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8055 - val_loss: 0.8101\n",
      "Epoch 397/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8049Epoch 00396: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8050 - val_loss: 0.8097\n",
      "Epoch 398/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8058Epoch 00397: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8057 - val_loss: 0.8109\n",
      "Epoch 399/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8062Epoch 00398: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8065 - val_loss: 0.8122\n",
      "Epoch 400/400\n",
      "16200/16500 [============================>.] - ETA: 0s - loss: 0.8091Epoch 00399: val_loss did not improve\n",
      "16500/16500 [==============================] - 4s - loss: 0.8093 - val_loss: 0.8138\n"
     ]
    }
   ],
   "source": [
    "history_raw = model.Fit(corrected_features_train, corrected_features_test,\n",
    "                epochs=400, batch_size=600, shuffle=True, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selu_results = deepcopy(history_raw.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_linear_results_5 = deepcopy(history_raw.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.86)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAFmCAYAAAB5iLH+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndhGRmvn/zZJjkDme4FMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ead9cc750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.scatter(range(len(linear_results)), linear_results)\n",
    "#plt.scatter(range(len(non_linear_results)), non_linear_results)\n",
    "#plt.scatter(range(len(non_linear_results_2)), non_linear_results_2)\n",
    "plt.scatter(range(len(selu_results)), selu_results)\n",
    "plt.scatter(range(len(history_raw.history['val_loss'])), history_raw.history['val_loss'])\n",
    "plt.ylim(0.8,0.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_317 (Dense)            (None, 400)               128400    \n",
      "_________________________________________________________________\n",
      "dense_318 (Dense)            (None, 260)               104260    \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 169)               44109     \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 109)               18530     \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 70)                7700      \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 45)                3195      \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 29)                1334      \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 18)                540       \n",
      "_________________________________________________________________\n",
      "dense_325 (Dense)            (None, 11)                209       \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 7)                 84        \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 4)                 32        \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 7)                 35        \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 11)                88        \n",
      "_________________________________________________________________\n",
      "dense_334 (Dense)            (None, 18)                216       \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 29)                551       \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 45)                1350      \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 70)                3220      \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 109)               7739      \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 169)               18590     \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 260)               44200     \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 400)               104400    \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 320)               128320    \n",
      "=================================================================\n",
      "Total params: 617,131\n",
      "Trainable params: 617,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.Summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_data = model.Predict(corrected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OP = []\n",
    "for i in range(len(etas)):\n",
    "    OP.append(mean(encoded_data[:,0][i*400:i*400+400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanj/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:14: MatplotlibDeprecationWarning: The set_axis_bgcolor function was deprecated in version 2.0. Use set_facecolor instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAGpCAYAAADcG3JYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmoJKREQSTUElIiKJpqASEZFE+/8Pk+8BFNfJjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e96d405d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the principal components\n",
    "colors = 200*['green', 'purple', 'orange', 'teal', 'black', 'red']\n",
    "\n",
    "#highlight the transition region if found\n",
    "plt.axvspan(0.700, 0.716, alpha=0.2, color='grey')\n",
    "plt.axvspan(0.716, 0.745, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(etas, array(OP), marker='o', linewidth=0.0, markersize=10, color = 'orange', alpha=1.0, markerfacecolor='None') \n",
    "\n",
    "\n",
    "#backround grid details\n",
    "axes = plt.gca()\n",
    "axes.grid(b = True, which = 'both', axis = 'both', color = 'gray', linestyle = '-', alpha = 0.5, linewidth = 0.5) \n",
    "axes.set_axis_bgcolor('white')  \n",
    "\n",
    "#font scpecifications\n",
    "title_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'heavy','size': 20}\n",
    "axis_label_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'normal','size': 20}                                                   \n",
    "\n",
    "#figure size and tick style\n",
    "plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "plt.rc('axes',edgecolor='black',linewidth=1)\n",
    "plt.tick_params(which='both', axis='both', color='black', length=4, width=0.5)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "#plt.yscale('log')\n",
    "#plt.ylim(0,0.3)\n",
    "\n",
    "plt.xlabel(r'$P_{1}$', y=3, fontsize=20, fontdict = axis_label_font)\n",
    "plt.ylabel(r'$P_{1}$', fontsize=20, fontdict = axis_label_font)\n",
    "\n",
    "#title and axis labels\n",
    "plt.tick_params(axis='both', labelsize=20)\n",
    "plt.title('Autoencoder', y=1.05, fontdict = title_font)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanj/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:14: MatplotlibDeprecationWarning: The set_axis_bgcolor function was deprecated in version 2.0. Use set_facecolor instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAGpCAYAAADcG3JYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0eabc7cc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the principal components\n",
    "colors = 200*['green', 'purple', 'orange', 'teal', 'black', 'red']\n",
    "\n",
    "#highlight the transition region if found\n",
    "plt.axvspan(0.700, 0.716, alpha=0.2, color='grey')\n",
    "plt.axvspan(0.716, 0.745, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(etas, array(OP), marker='o', linewidth=0.0, markersize=10, color = 'orange', alpha=1.0, markerfacecolor='None') \n",
    "\n",
    "\n",
    "#backround grid details\n",
    "axes = plt.gca()\n",
    "axes.grid(b = True, which = 'both', axis = 'both', color = 'gray', linestyle = '-', alpha = 0.5, linewidth = 0.5) \n",
    "axes.set_axis_bgcolor('white')  \n",
    "\n",
    "#font scpecifications\n",
    "title_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'heavy','size': 20}\n",
    "axis_label_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'normal','size': 20}                                                   \n",
    "\n",
    "#figure size and tick style\n",
    "plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "plt.rc('axes',edgecolor='black',linewidth=1)\n",
    "plt.tick_params(which='both', axis='both', color='black', length=4, width=0.5)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "#plt.yscale('log')\n",
    "#plt.ylim(0,0.3)\n",
    "\n",
    "plt.xlabel(r'$P_{1}$', y=3, fontsize=20, fontdict = axis_label_font)\n",
    "plt.ylabel(r'$P_{1}$', fontsize=20, fontdict = axis_label_font)\n",
    "\n",
    "#title and axis labels\n",
    "plt.tick_params(axis='both', labelsize=20)\n",
    "plt.title('Autoencoder', y=1.05, fontdict = title_font)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the principal components\n",
    "colors = 200*['green', 'purple', 'orange', 'teal', 'black', 'red']\n",
    "\n",
    "plt.plot(encoded_data[:,0][:skip], encoded_data[:,1][:skip], marker='o', linewidth=0.0, markersize=8, color = 'orange', alpha=0.1) #markerfacecolor='None')   \n",
    "plt.plot(encoded_data[:,0][skip:], encoded_data[:,1][skip:], marker='o', linewidth=0.0, markersize=8, color = 'purple', alpha=0.1, markerfacecolor='None')  \n",
    "#plt.plot(encoded_data[:,0][:skip], encoded_data[:,1][:skip], marker='o', linewidth=0.0, markersize=4, color = 'orange', alpha=0.1) #markerfacecolor='None')   \n",
    "\n",
    "#backround grid details\n",
    "axes = plt.gca()\n",
    "axes.grid(b = True, which = 'both', axis = 'both', color = 'gray', linestyle = '-', alpha = 0.5, linewidth = 0.5) \n",
    "axes.set_axis_bgcolor('white')  \n",
    "\n",
    "#font scpecifications\n",
    "title_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'heavy','size': 20}\n",
    "axis_label_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'normal','size': 20}                                                   \n",
    "\n",
    "#figure size and tick style\n",
    "plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "plt.rc('axes',edgecolor='black',linewidth=1)\n",
    "plt.tick_params(which='both', axis='both', color='black', length=4, width=0.5)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "#plt.xlim(-1.2,20)\n",
    "#plt.ylim(-1.2,20)\n",
    "\n",
    "plt.xlabel(r'$P_{1}$', y=3, fontsize=20, fontdict = axis_label_font)\n",
    "plt.ylabel(r'$P_{1}$', fontsize=20, fontdict = axis_label_font)\n",
    "\n",
    "#title and axis labels\n",
    "plt.tick_params(axis='both', labelsize=20)\n",
    "plt.title('Autoencoder', y=1.05, fontdict = title_font)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.autoencoder.history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
